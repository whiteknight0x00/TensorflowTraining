{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn to implement transfer learning using a pre-trained model in Keras.\n",
    "\n",
    "#### Learning objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    " - Import necessary libraries and load the dataset.\n",
    " - Load a pre-trained model, VGG16, excluding the top layers.\n",
    " - Add new layers on top of the base model and compile the model.\n",
    " - Train the model on the new dataset.\n",
    " - Unfreeze some of the layers of the pre-trained model and fine-tune them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-Step Guide:\n",
    "Step 1: Setup the Environment\n",
    "Before we start, make sure to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as tensorflow.keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.applications import VGG16\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Flatten\n",
    "# Explanation:\n",
    "# tensorflow is the main library for machine learning in Python.\n",
    "# Sequential is used to create a model with a linear stack of layers.\n",
    "# Dense and Flatten are types of layers that we will use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model pre-trained on ImageNet\n",
    "base_model = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3: Create and Compile the Model \n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images created in 'sample_data/'\n"
     ]
    }
   ],
   "source": [
    "# Create Placeholder Images\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('sample_data/class_a', exist_ok = True)\n",
    "os.makedirs('sample_data/class_b', exist_ok = True)\n",
    "\n",
    "# Create 10 sample images for each class\n",
    "for i in range(10):\n",
    "    # Create a blank white image for class_a\n",
    "    # Return a new array of given shape and type, filled with ones.\n",
    "    img = Image.fromarray(np.ones((224, 224, 3), dtype = np.uint8) * 255)\n",
    "    img.save(f'sample_data/class_a/img_{i}.jpg')\n",
    "\n",
    "    # Create a blank black image for class_b\n",
    "    # @overload\n",
    "    # def zeros(\n",
    "    #     shape: _ShapeLike,\n",
    "    #     dtype: None = ...,\n",
    "    #     order: _OrderCF = ...,\n",
    "    #     *,\n",
    "    #     device: None | L[\"cpu\"] = ...,\n",
    "    #     like: None | _SupportsArrayFunc = ...,\n",
    "    # ) -> NDArray[float64]: ...\n",
    "    # @overload\n",
    "    # def zeros(\n",
    "    #     shape: _ShapeLike,\n",
    "    #     dtype: _DTypeLike[_SCT],\n",
    "    #     order: _OrderCF = ...,\n",
    "    #     *,\n",
    "    #     device: None | L[\"cpu\"] = ...,\n",
    "    #     like: None | _SupportsArrayFunc = ...,\n",
    "    # ) -> NDArray[_SCT]: ...\n",
    "    # @overload\n",
    "    # def zeros(\n",
    "    #     shape: _ShapeLike,\n",
    "    #     dtype: DTypeLike,\n",
    "    #     order: _OrderCF = ...,\n",
    "    #     *,\n",
    "    #     device: None | L[\"cpu\"] = ...,\n",
    "    #     like: None | _SupportsArrayFunc = ...,\n",
    "    # ) -> NDArray[Any]: ...\n",
    "    # has been overloaded\n",
    "    img = Image.fromarray(np.zeros((224, 224, 3), dtype = np.uint8))\n",
    "    img.save(f'sample_data/class_b/img_{i}.jpg')\n",
    "\n",
    "print(\"Sample images created in 'sample_data/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.5000 - loss: 0.6818\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5000 - loss: 0.4138\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5000 - loss: 0.3712\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3459\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.3451\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.3447\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.3442\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3437\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 22:33:37.226664: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3433\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3428\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the Model¶\n",
    "# Load the preprocess the dataset\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "def load_and_preprocess_image(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels = 3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = image / 255.0 # Rescale the image\n",
    "    return image\n",
    "\n",
    "# Define the function to load the dataset\n",
    "def load_dataset(directory):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            file_paths.append(os.path.join(class_dir, file_name))\n",
    "            labels.append(label)\n",
    "    return file_paths, labels, class_names\n",
    "\n",
    "# Load the dataset\n",
    "directory = 'sample_data'\n",
    "file_paths, labels, class_names = load_dataset(directory)\n",
    "\n",
    "# Create a tf.data.Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "dataset = dataset.map(lambda x, y: (load_and_preprocess_image(x), y), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(file_paths))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Verify if the dataset has loaded images correctly\n",
    "print(f\"Found {len(file_paths)} images belonging to {len(class_names)} classes.\")\n",
    "\n",
    "# Define a simple model for demonstration purposes\n",
    "model = Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (224, 224, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "if len(file_paths) > 0:\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot\n",
    "Since ImageDataGenerator is deprecated, we will use tf.data to load and preprocess the dataset. Here's how you can achieve the same functionality using tf.data:\n",
    "\n",
    "Load and preprocess the dataset.\n",
    "Create a tf.data.Dataset object.\n",
    "Train the model.\n",
    "Here's the updated code:\n",
    "\n",
    "1 vulnerability\n",
    "In this code:\n",
    "\n",
    "The load_and_preprocess_image function loads and preprocesses the images by reading the file, decoding the JPEG image, resizing it to the target size, and rescaling the pixel values.\n",
    "The load_dataset function loads the dataset from the specified directory and returns the file paths, labels, and class names.\n",
    "The tf.data.Dataset object is created from the file paths and labels, and the images are loaded and preprocessed using the load_and_preprocess_image function.\n",
    "The dataset is split into training and validation sets.\n",
    "A simple model is defined, compiled, and trained using the training and validation datasets.\n",
    "Similar code found with 2 license types - View matches\n",
    "whiteknight0x00\n",
    "whats the meaning of channels = 3?\n",
    "\n",
    "GitHub Copilot\n",
    "In the context of image processing, the term \"channels\" refers to the different color components that make up an image. Each channel represents a specific color in the image. The most common color channels are:\n",
    "\n",
    "1 channel: Grayscale image (only intensity values, no color information).\n",
    "3 channels: RGB image (Red, Green, and Blue channels).\n",
    "4 channels: RGBA image (Red, Green, Blue, and Alpha channels, where Alpha represents transparency).\n",
    "When you see channels=3, it means that the image has three color channels, which typically correspond to the RGB color model. Each pixel in the image is represented by three values: one for the red component, one for the green component, and one for the blue component.\n",
    "\n",
    "For example, in the following code snippet from your active file, the decode_jpeg function is used to decode a JPEG image, and the channels parameter specifies the number of color channels in the output image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_jpeg = tf_export(\n",
    "#     'io.decode_jpeg',\n",
    "#     'image.decode_jpeg',\n",
    "#     v1=['io.decode_jpeg', 'image.decode_jpeg'])(\n",
    "#         dispatch.add_dispatch_support(gen_image_ops.decode_jpeg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set channels=3, the decoded image will have three color channels (RGB). If you set channels=1, the decoded image will be a grayscale image with only one channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Visualize Training and Validation Loss\n",
    "\n",
    "**Objective:** Plot the training and validation loss to observe the learning process of the model.\n",
    "\n",
    "**Instructions:**\n",
    "1. Modify the training code to include validation data.\n",
    "2. Plot the training and validation loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whiteknight/.local/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614ms/step - accuracy: 0.0000e+00 - loss: 0.7963\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 0.4456\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.3844\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 22:46:59.212360: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Users/whiteknight/.local/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.3467\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.3458\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3454\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.3450\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3446\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.3442\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.3438\n",
      "{'accuracy': [0.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'loss': [0.7962669134140015, 0.4455734193325043, 0.3843790292739868, 0.3467053771018982, 0.3458406329154968, 0.34543171525001526, 0.3450325131416321, 0.34462469816207886, 0.34420984983444214, 0.3437888026237488]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Plot training and validation loss\u001b[39;00m\n\u001b[32m     66\u001b[39m plt.plot(history.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m], label = \u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m plt.plot(\u001b[43mhistory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label = \u001b[33m'\u001b[39m\u001b[33mValidation Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     68\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mTraining and Validation Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     69\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "def load_and_preprocess_image(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels = 3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = image / 255.0 # Rescale the image\n",
    "    return image\n",
    "\n",
    "# Define the function to load the dataset\n",
    "def load_dataset(directory):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            file_paths.append(os.path.join(class_dir, file_name))\n",
    "            labels.append(label)\n",
    "    return file_paths, labels, class_names\n",
    "\n",
    "# Load the dataset\n",
    "directory = 'sample_data'\n",
    "file_paths, labels, class_names = load_dataset(directory)\n",
    "\n",
    "# Create a tf.data.Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "dataset = dataset.map(lambda x, y: (load_and_preprocess_image(x), y), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(file_paths))\n",
    "print(train_size)\n",
    "print(len(file_paths))\n",
    "val_size = len(file_paths) - train_size\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "\n",
    "for i in val_dataset:\n",
    "    print(f\"val_dataset:{i}\\n\")\n",
    "\n",
    "\n",
    "# Ensure that the validation dataset is not empty\n",
    "if val_size == 0:\n",
    "    raise ValueError(\"Validation dataset is empty. Please ensure there are enough samples to create a validation set.\")\n",
    "\n",
    "# Define a simple model for demonstration purposes\n",
    "model = Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (224, 224, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# TODO: So there is an error in the code, which I am not able to figure out.\n",
    "# The error is: KeyError: 'val_loss'\n",
    "# I am not for sure if the error is caused by the pre-trained model.\n",
    "# I think the error is caused is caused by empty validation dataset, So I will figure it out tomorrow.\n",
    "# End of the day.\n",
    "# Date: 13/03/2025 10:50 PM\n",
    "history = model.fit(train_dataset, validation_data = val_dataset, epochs=10)\n",
    "\n",
    "\n",
    "print(history.history)\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
