{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Set up the environment \n",
    "\n",
    "- Define the neural network model \n",
    "\n",
    "- Define the Loss Function and Optimizer \n",
    "\n",
    "- Implement the custom training loop \n",
    "\n",
    "- Enhance the custom training loop by adding an accuracy metric to monitor model performance \n",
    "\n",
    "- Implement a custom callback to log additional metrics and information during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic custom training loop: \n",
    "\n",
    "#### 1. Set Up the Environment:\n",
    "\n",
    "- Import necessary libraries. \n",
    "\n",
    "- Load and preprocess the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.api.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.api.models import Model, Sequential\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.losses import categorical_crossentropy\n",
    "from keras.api.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.api.applications import VGG16\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function and Optimizer: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "- Use the Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the Custom Training Loop: \n",
    "\n",
    "- Iterate over the dataset for a specified number of epochs. \n",
    "- Compute the loss and apply gradients to update the model's weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "epochs = 2\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        '''\n",
    "          \"\"\"Record operations for automatic differentiation.\n",
    "\n",
    "  Operations are recorded if they are executed within this context manager and\n",
    "  at least one of their inputs is being \"watched\".\n",
    "\n",
    "  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n",
    "  where `trainable=True` is default in both cases) are automatically watched.\n",
    "  Tensors can be manually watched by invoking the `watch` method on this context\n",
    "  manager.\n",
    "\n",
    "  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n",
    "  be computed as:\n",
    "\n",
    "  >>> x = tf.constant(3.0)\n",
    "  >>> with tf.GradientTape() as g:\n",
    "  ...   g.watch(x)\n",
    "  ...   y = x * x\n",
    "  >>> dy_dx = g.gradient(y, x)\n",
    "  >>> print(dy_dx)\n",
    "  tf.Tensor(6.0, shape=(), dtype=float32)\n",
    "\n",
    "  GradientTapes can be nested to compute higher-order derivatives. For example,\n",
    "\n",
    "  >>> x = tf.constant(5.0)\n",
    "  >>> with tf.GradientTape() as g:\n",
    "  ...   g.watch(x)\n",
    "  ...   with tf.GradientTape() as gg:\n",
    "  ...     gg.watch(x)\n",
    "  ...     y = x * x\n",
    "  ...   dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n",
    "  >>> d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n",
    "  >>> print(dy_dx)\n",
    "  tf.Tensor(10.0, shape=(), dtype=float32)\n",
    "  >>> print(d2y_dx2)\n",
    "  tf.Tensor(2.0, shape=(), dtype=float32)\n",
    "\n",
    "  By default, the resources held by a GradientTape are released as soon as\n",
    "  GradientTape.gradient() method is called. To compute multiple gradients over\n",
    "  the same computation, create a persistent gradient tape. This allows multiple\n",
    "  calls to the gradient() method as resources are released when the tape object\n",
    "  is garbage collected. For example:\n",
    "\n",
    "  >>> x = tf.constant(3.0)\n",
    "  >>> with tf.GradientTape(persistent=True) as g:\n",
    "  ...   g.watch(x)\n",
    "  ...   y = x * x\n",
    "  ...   z = y * y\n",
    "  >>> dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n",
    "  >>> print(dz_dx)\n",
    "  tf.Tensor(108.0, shape=(), dtype=float32)\n",
    "  >>> dy_dx = g.gradient(y, x)\n",
    "  >>> print(dy_dx)\n",
    "  tf.Tensor(6.0, shape=(), dtype=float32)\n",
    "\n",
    "  By default GradientTape will automatically watch any trainable variables that\n",
    "  are accessed inside the context. If you want fine grained control over which\n",
    "  variables are watched you can disable automatic tracking by passing\n",
    "  `watch_accessed_variables=False` to the tape constructor:\n",
    "\n",
    "  >>> x = tf.Variable(2.0)\n",
    "  >>> w = tf.Variable(5.0)\n",
    "  >>> with tf.GradientTape(\n",
    "  ...     watch_accessed_variables=False, persistent=True) as tape:\n",
    "  ...   tape.watch(x)\n",
    "  ...   y = x ** 2  # Gradients will be available for `x`.\n",
    "  ...   z = w ** 3  # No gradients will be available as `w` isn't being watched.\n",
    "  >>> dy_dx = tape.gradient(y, x)\n",
    "  >>> print(dy_dx)\n",
    "  tf.Tensor(4.0, shape=(), dtype=float32)\n",
    "  >>> # No gradients will be available as `w` isn't being watched.\n",
    "  >>> dz_dw = tape.gradient(z, w)\n",
    "  >>> print(dz_dw)\n",
    "  None\n",
    "\n",
    "  Note that when using models you should ensure that your variables exist when\n",
    "  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n",
    "  first iteration not have any gradients:\n",
    "\n",
    "  ```python\n",
    "  a = tf.keras.layers.Dense(32)\n",
    "  b = tf.keras.layers.Dense(32)\n",
    "\n",
    "  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
    "                             # `a.variables` will return an empty list and the\n",
    "                             # tape will not be watching anything.\n",
    "    result = b(a(inputs))\n",
    "    tape.gradient(result, a.variables)  # The result of this computation will be\n",
    "                                        # a list of `None`s since a's variables\n",
    "                                        # are not being watched.\n",
    "  ```\n",
    "\n",
    "  Note that only tensors with real or complex dtypes are differentiable.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, persistent=False, watch_accessed_variables=True):\n",
    "    \"\"\"Creates a new GradientTape.\n",
    "\n",
    "    Args:\n",
    "      persistent: Boolean controlling whether a persistent gradient tape\n",
    "        is created. False by default, which means at most one call can\n",
    "        be made to the gradient() method on this object.\n",
    "      watch_accessed_variables: Boolean controlling whether the tape will\n",
    "        automatically `watch` any (trainable) variables accessed while the tape\n",
    "        is active. Defaults to True meaning gradients can be requested from any\n",
    "        result computed in the tape derived from reading a trainable `Variable`.\n",
    "        If False users must explicitly `watch` any `Variable`s they want to\n",
    "        request gradients from.\n",
    "    \"\"\"\n",
    "    self._tape = None\n",
    "    self._persistent = persistent\n",
    "    self._watch_accessed_variables = watch_accessed_variables\n",
    "    self._watched_variables = ()\n",
    "    self._recording = False\n",
    "        '''\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)    # Foward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits) # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        '''\n",
    "        \"\"\"Computes the gradient using operations recorded in context of this tape.\n",
    "\n",
    "    Note: Unless you set `persistent=True` a GradientTape can only be used to\n",
    "    compute one set of gradients (or jacobians).\n",
    "\n",
    "    In addition to Tensors, gradient also supports RaggedTensors. For example,\n",
    "\n",
    "    >>> x = tf.ragged.constant([[1.0, 2.0], [3.0]])\n",
    "    >>> with tf.GradientTape() as g:\n",
    "    ...   g.watch(x)\n",
    "    ...   y = x * x\n",
    "    >>> g.gradient(y, x)\n",
    "    <tf.RaggedTensor [[2.0, 4.0], [6.0]]>\n",
    "\n",
    "    Args:\n",
    "      target: a list or nested structure of Tensors or Variables or\n",
    "        CompositeTensors to be differentiated.\n",
    "      sources: a list or nested structure of Tensors or Variables or\n",
    "        CompositeTensors. `target` will be differentiated against elements in\n",
    "        `sources`.\n",
    "      output_gradients: a list of gradients, one for each differentiable\n",
    "        element of target. Defaults to None.\n",
    "      unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
    "        alters the value which will be returned if the target and sources are\n",
    "        unconnected. The possible values and effects are detailed in\n",
    "        'UnconnectedGradients' and it defaults to 'none'.\n",
    "\n",
    "    Returns:\n",
    "      a list or nested structure of Tensors (or IndexedSlices, or None, or\n",
    "      CompositeTensor), one for each element in `sources`. Returned structure\n",
    "      is the same as the structure of `sources`.\n",
    "\n",
    "    Raises:\n",
    "      RuntimeError: If called on a used, non-persistent tape.\n",
    "      RuntimeError: If called inside the context of the tape.\n",
    "      TypeError: If the target is a None object.\n",
    "      ValueError: If the target is a variable or if unconnected gradients is\n",
    "       called with an unknown value.\n",
    "    \"\"\"\n",
    "        '''\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        '''\n",
    "        \"\"\"Update traininable variables according to provided gradient values.\n",
    "\n",
    "        `grads` should be a list of gradient tensors\n",
    "        with 1:1 mapping to the list of variables the optimizer was built with.\n",
    "\n",
    "        `trainable_variables` can be provided\n",
    "        on the first call to build the optimizer.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Training loss (for one batch) at step {step}: {loss_value.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric:\n",
    "\n",
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.api.models import Sequential \n",
    "from keras.api.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the loss function, optimizer, and metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function, Optimizer, and Metic\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()  # Metric to monitor the training and validation accuracy\n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Foward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update the model's weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update the training metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "\n",
    "    # Reset the metric at the end of the epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging: \n",
    "\n",
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.api.models import Sequential \n",
    "from keras.api.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n",
    "\n",
    "from keras.api.callbacks import Callback\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch}. Loss = {logs.get(\"loss\")} Accuracy = {logs.get(\"accuracy\")}')\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Foward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update the model's weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update the training metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 batches\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "\n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    # Reset the metric at the end of the epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(28, 28))\n",
    "\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function. \n",
    "\n",
    "Each hidden layer takes the output of the previous layer as its input.\n",
    "\n",
    "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification.\n",
    "\n",
    "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks. \n",
    "\n",
    "`loss='binary_crossentropy'` specifies the loss function for binary classification problems. \n",
    "\n",
    "`metrics=['accuracy']` tells Keras to evaluate the model using accuracy during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(20,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train` and `y_train` are placeholders for your actual training data. \n",
    "\n",
    "`model.fit` trains the model for a specified number of epochs and batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200,1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss:{loss}')\n",
    "print(f'Test accuracy:{accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.evaluate` computes the loss and accuracy of the model on test data. \n",
    "\n",
    "`X_test` and `y_test` are placeholders for your actual test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises \n",
    "\n",
    "### Exercise 1: Basic Custom Training Loop \n",
    "\n",
    "#### Objective: Implement a basic custom training loop to train a simple neural network on the MNIST dataset. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.api.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.api.models import Model, Sequential\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.losses import categorical_crossentropy\n",
    "from keras.api.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.api.applications import VGG16\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric \n",
    "\n",
    "#### Objective: Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, and optimizer. \n",
    "\n",
    "2. Add Sparse Categorical Accuracy as a metric. \n",
    "\n",
    "3. Implement the custom training loop with accuracy tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Flatten\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        accuracy_metric.update_state(y_batch, logits)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}, Accuracy: {accuracy_metric.result().numpy()}')\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging \n",
    "\n",
    "#### Objective: Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "1. Set up the environment and define the model, loss function, optimizer, and metric. \n",
    "\n",
    "2. Create a custom callback to log additional metrics at the end of each epoch. \n",
    "\n",
    "3. Implement the custom training loop with the custom callback. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = keras.api.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.api.optimizers.Adam()\n",
    "accuracy_metric = keras.api.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'End of epoch {epoch + 1}. Loss = {logs.get(\"loss\")}, Accuracy = {logs.get(\"sparse_categorical_accuracy\")}')\n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        accuracy_metric.update_state(y_batch, logits)\n",
    "\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning \n",
    "\n",
    "#### Enhancement: Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "#### Additional Instructions:\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from keras import Sequential\n",
    "from keras.api.layers import Dense\n",
    "from keras.api.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(build_model, \n",
    "                        objective='val_accuracy', \n",
    "                        max_trials=10,  # Set the number of trials\n",
    "                        executions_per_trial=1, # Set how many executions per trail\n",
    "                        directory='tuner_results',  # Directory for saving logs\n",
    "                        project_name='hyperparam_tuning',)\n",
    "'''\n",
    "    \"\"\"Random search tuner.\n",
    "\n",
    "    Args:\n",
    "        hypermodel: Instance of `HyperModel` class (or callable that takes\n",
    "            hyperparameters and returns a Model instance). It is optional when\n",
    "            `Tuner.run_trial()` is overridden and does not use\n",
    "            `self.hypermodel`.\n",
    "        objective: A string, `keras_tuner.Objective` instance, or a list of\n",
    "            `keras_tuner.Objective`s and strings. If a string, the direction of\n",
    "            the optimization (min or max) will be inferred. If a list of\n",
    "            `keras_tuner.Objective`, we will minimize the sum of all the\n",
    "            objectives to minimize subtracting the sum of all the objectives to\n",
    "            maximize. The `objective` argument is optional when\n",
    "            `Tuner.run_trial()` or `HyperModel.fit()` returns a single float as\n",
    "            the objective to minimize.\n",
    "        max_trials: Integer, the total number of trials (model configurations)\n",
    "            to test at most. Note that the oracle may interrupt the search\n",
    "            before `max_trial` models have been tested if the search space has\n",
    "            been exhausted. Defaults to 10.\n",
    "        seed: Optional integer, the random seed.\n",
    "        hyperparameters: Optional `HyperParameters` instance. Can be used to\n",
    "            override (or register in advance) hyperparameters in the search\n",
    "            space.\n",
    "        tune_new_entries: Boolean, whether hyperparameter entries that are\n",
    "            requested by the hypermodel but that were not specified in\n",
    "            `hyperparameters` should be added to the search space, or not. If\n",
    "            not, then the default value for these parameters will be used.\n",
    "            Defaults to True.\n",
    "        allow_new_entries: Boolean, whether the hypermodel is allowed to\n",
    "            request hyperparameter entries not listed in `hyperparameters`.\n",
    "            Defaults to True.\n",
    "        max_retries_per_trial: Integer. Defaults to 0. The maximum number of\n",
    "            times to retry a `Trial` if the trial crashed or the results are\n",
    "            invalid.\n",
    "        max_consecutive_failed_trials: Integer. Defaults to 3. The maximum\n",
    "            number of consecutive failed `Trial`s. When this number is reached,\n",
    "            the search will be stopped. A `Trial` is marked as failed when none\n",
    "            of the retries succeeded.\n",
    "        **kwargs: Keyword arguments relevant to all `Tuner` subclasses.\n",
    "            Please see the docstring for `Tuner`.\n",
    "    \"\"\"\n",
    "'''\n",
    "# Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n",
    "\n",
    "# Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        \"\"\"Returns the best hyperparameters, as determined by the objective.\n",
    "\n",
    "        This method can be used to reinstantiate the (untrained) best model\n",
    "        found during the search process.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        best_hp = tuner.get_best_hyperparameters()[0]\n",
    "        model = tuner.hypermodel.build(best_hp)\n",
    "        ```\n",
    "\n",
    "        Args:\n",
    "            num_trials: Optional number of `HyperParameters` objects to return.\n",
    "\n",
    "        Returns:\n",
    "            List of `HyperParameter` objects sorted from the best to the worst.\n",
    "        \"\"\"\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values, # Hyperparameters tuned in this trial\n",
    "            \"score\": None   # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\"\n",
    " ```   \n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
